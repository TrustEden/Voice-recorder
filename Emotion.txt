
Text icon
Emotion.txt

Page
1
/
1
100%
ï»¿"""
Eden Emotion Detection Plugin
Real-time voice emotion analysis with contextual risk assessment.
Integrates with consent and training systems for enhanced metadata.


File: src/plugins/emotion/plugin.py
"""


import os
import time
import json
import logging
import threading
import queue
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from pathlib import Path
from collections import deque
import torch
import torchaudio
import librosa
from datetime import datetime


# Try to import emotion detection libraries
try:
    import transformers
    from transformers import pipeline, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("Transformers not available, using fallback emotion detection")


try:
    import opensmile
    OPENSMILE_AVAILABLE = True
except ImportError:
    OPENSMILE_AVAILABLE = False
    logging.warning("OpenSMILE not available, acoustic features will be limited")


from src.core.audio_recorder import AudioChunk, PluginInterface




@dataclass
class EmotionConfig:
    """Configuration for emotion detection plugin"""
    enabled: bool = True
    model_name: str = "facebook/wav2vec2-large-xlsr-53-emotional-speech"
    device: str = "cuda"  # cuda or cpu
    
    # Detection settings
    min_audio_duration: float = 1.0  # Minimum seconds for reliable emotion detection
    max_audio_duration: float = 5.0  # Maximum seconds to process
    confidence_threshold: float = 0.6  # Minimum confidence for emotion labeling
    
    # Emotion categories (expandable)
    emotion_categories: List[str] = field(default_factory=lambda: [
        "neutral", "calm", "happy", "excited", "angry", "frustrated", 
        "sad", "fear", "anxiety", "stress", "pain", "surprise"
    ])
    
    # Risk assessment
    risk_emotions: List[str] = field(default_factory=lambda: [
        "angry", "frustrated", "anxiety", "stress", "pain", "fear"
    ])
    high_risk_threshold: float = 0.8  # Confidence threshold for high-risk emotions
    
    # Processing settings
    buffer_overlap: float = 0.5  # Overlap between processing windows
    smoothing_window: int = 5  # Number of predictions to smooth over
    
    # Output settings
    save_emotion_timeline: bool = True
    emotion_storage_path: str = "data/emotions"




@dataclass
class EmotionPrediction:
    """Individual emotion prediction with metadata"""
    prediction_id: str
    session_id: str
    speaker_id: str
    timestamp: float
    duration: float
    
    # Primary emotion prediction
    primary_emotion: str
    primary_confidence: float
    
    # All emotion scores
    emotion_scores: Dict[str, float]
    
    # Risk assessment
    risk_level: str  # low, medium, high
    risk_score: float
    risk_factors: List[str]
    
    # Acoustic features
    acoustic_features: Dict[str, float] = field(default_factory=dict)
    
    # Context
    consent_token: str = ""
    is_speech: bool = True
    voice_activity_score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            'prediction_id': self.prediction_id,
            'session_id': self.session_id,
            'speaker_id': self.speaker_id,
            'timestamp': self.timestamp,
            'duration': self.duration,
            'primary_emotion': self.primary_emotion,
            'primary_confidence': self.primary_confidence,
            'emotion_scores': self.emotion_scores,
            'risk_level': self.risk_level,
            'risk_score': self.risk_score,
            'risk_factors': self.risk_factors,
            'acoustic_features': self.acoustic_features,
            'consent_token': self.consent_token,
            'is_speech': self.is_speech,
            'voice_activity_score': self.voice_activity_score
        }




class TransformerEmotionDetector:
    """Transformer-based emotion detection using Wav2Vec2"""
    
    def __init__(self, model_name: str, device: str = "cuda"):
        self.model_name = model_name
        self.device = device if torch.cuda.is_available() else "cpu"
        self.model = None
        self.processor = None
        self.classifier = None
        self.logger = logging.getLogger(__name__)
        
        if TRANSFORMERS_AVAILABLE:
            self._load_model()
        else:
            self.logger.error("Transformers not available - using fallback emotion detection")
    
    def _load_model(self):
        """Load transformer model for emotion detection"""
        try:
            self.logger.info(f"Loading emotion model: {self.model_name} on {self.device}")
            
            # Try to load as pre-trained emotion classifier
            try:
                self.classifier = pipeline(
                    "audio-classification",
                    model=self.model_name,
                    device=0 if self.device == "cuda" and torch.cuda.is_available() else -1
                )
                self.logger.info("Loaded emotion classifier pipeline")
                return
            except:
                pass
            
            # Fallback: Load Wav2Vec2 model manually
            try:
                self.processor = Wav2Vec2Processor.from_pretrained(self.model_name)
                self.model = Wav2Vec2ForSequenceClassification.from_pretrained(self.model_name)
                self.model.to(self.device)
                self.model.eval()
                self.logger.info("Loaded Wav2Vec2 emotion model")
                return
            except:
                pass
            
            # Final fallback: Use a general audio classification model
            try:
                self.classifier = pipeline(
                    "audio-classification",
                    model="facebook/wav2vec2-base-960h",
                    device=0 if self.device == "cuda" and torch.cuda.is_available() else -1
                )
                self.logger.warning("Using fallback audio classification model")
            except Exception as e:
                self.logger.error(f"Failed to load any emotion model: {e}")
                self.classifier = None
                
        except Exception as e:
            self.logger.error(f"Failed to load emotion model: {e}")
    
    def predict_emotion(self, audio_data: np.ndarray, sample_rate: int = 16000) -> Dict[str, float]:
        """
        Predict emotions from audio data
        Returns: Dictionary of emotion -> confidence scores
        """
        if not self.classifier and not self.model:
            return self._fallback_emotion_detection(audio_data, sample_rate)
        
        try:
            # Ensure audio is the right format
            if audio_data.dtype != np.float32:
                audio_data = audio_data.astype(np.float32)
            
            # Resample if needed
            if sample_rate != 16000:
                audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
            
            # Use pipeline classifier if available
            if self.classifier:
                results = self.classifier(audio_data, sampling_rate=16000)
                
                # Convert to standard emotion format
                emotion_scores = {}
                for result in results:
                    label = result['label'].lower()
                    score = result['score']
                    
                    # Map labels to standard emotions
                    mapped_emotion = self._map_label_to_emotion(label)
                    emotion_scores[mapped_emotion] = score
                
                return emotion_scores
            
            # Use manual Wav2Vec2 model
            elif self.model and self.processor:
                inputs = self.processor(
                    audio_data, 
                    sampling_rate=16000, 
                    return_tensors="pt", 
                    padding=True
                )
                
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                    predictions = predictions.cpu().numpy()[0]
                
                # Map to emotion categories (model-dependent)
                emotion_categories = ["neutral", "happy", "sad", "angry", "fear", "surprise"]
                emotion_scores = {}
                for i, emotion in enumerate(emotion_categories[:len(predictions)]):
                    emotion_scores[emotion] = float(predictions[i])
                
                return emotion_scores
            
        except Exception as e:
            self.logger.error(f"Emotion prediction failed: {e}")
            return self._fallback_emotion_detection(audio_data, sample_rate)
    
    def _map_label_to_emotion(self, label: str) -> str:
        """Map model labels to standard emotion categories"""
        # Common mappings from various models
        label_mappings = {
            # Common emotion labels
            "happy": "happy",
            "joy": "happy",
            "positive": "happy",
            "excited": "excited",
            "angry": "angry",
            "anger": "angry",
            "mad": "angry",
            "sad": "sad",
            "sadness": "sad",
            "depression": "sad",
            "fear": "fear",
            "scared": "fear",
            "anxiety": "anxiety",
            "anxious": "anxiety",
            "worried": "anxiety",
            "neutral": "neutral",
            "calm": "calm",
            "relaxed": "calm",
            "surprise": "surprise",
            "surprised": "surprise",
            "stress": "stress",
            "stressed": "stress",
            "frustrated": "frustrated",
            "frustration": "frustrated",
            "pain": "pain",
            "hurt": "pain"
        }
        
        # Check for direct mapping
        if label in label_mappings:
            return label_mappings[label]
        
        # Check for partial matches
        for key, emotion in label_mappings.items():
            if key in label or label in key:
                return emotion
        
        # Default to neutral for unknown labels
        return "neutral"
    
    def _fallback_emotion_detection(self, audio_data: np.ndarray, sample_rate: int) -> Dict[str, float]:
        """
        Fallback emotion detection using acoustic features
        """
        try:
            # Extract basic acoustic features
            features = self._extract_acoustic_features(audio_data, sample_rate)
            
            # Simple heuristic-based emotion detection
            emotions = {}
            
            # Energy-based detection
            if features.get('rms_energy', 0) > 0.1:
                if features.get('spectral_centroid', 0) > 2000:
                    emotions['excited'] = 0.7
                    emotions['happy'] = 0.6
                else:
                    emotions['angry'] = 0.6
                    emotions['frustrated'] = 0.5
            else:
                emotions['calm'] = 0.7
                emotions['neutral'] = 0.6
            
            # Pitch-based detection
            if features.get('f0_mean', 0) > 200:
                emotions['excited'] = emotions.get('excited', 0) + 0.2
                emotions['anxiety'] = 0.4
            elif features.get('f0_std', 0) > 50:
                emotions['stress'] = 0.5
                emotions['anxiety'] = 0.6
            
            # Normalize scores
            total = sum(emotions.values())
            if total > 0:
                emotions = {k: v/total for k, v in emotions.items()}
            
            return emotions
            
        except Exception as e:
            self.logger.error(f"Fallback emotion detection failed: {e}")
            return {"neutral": 1.0}
    
    def _extract_acoustic_features(self, audio_data: np.ndarray, sample_rate: int) -> Dict[str, float]:
        """Extract basic acoustic features for emotion analysis"""
        try:
            features = {}
            
            # Energy features
            features['rms_energy'] = float(np.sqrt(np.mean(audio_data**2)))
            features['zero_crossing_rate'] = float(np.mean(librosa.feature.zero_crossing_rate(audio_data)))
            
            # Spectral features
            stft = librosa.stft(audio_data)
            magnitude = np.abs(stft)
            
            features['spectral_centroid'] = float(np.mean(librosa.feature.spectral_centroid(S=magnitude, sr=sample_rate)))
            features['spectral_bandwidth'] = float(np.mean(librosa.feature.spectral_bandwidth(S=magnitude, sr=sample_rate)))
            features['spectral_rolloff'] = float(np.mean(librosa.feature.spectral_rolloff(S=magnitude, sr=sample_rate)))
            
            # Pitch features
            try:
                f0, voiced_flag, voiced_probs = librosa.pyin(audio_data, fmin=80, fmax=400, sr=sample_rate)
                f0_clean = f0[voiced_flag]
                if len(f0_clean) > 0:
                    features['f0_mean'] = float(np.mean(f0_clean))
                    features['f0_std'] = float(np.std(f0_clean))
                else:
                    features['f0_mean'] = 0.0
                    features['f0_std'] = 0.0
            except:
                features['f0_mean'] = 0.0
                features['f0_std'] = 0.0
            
            # MFCC features (summary statistics)
            try:
                mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)
                features['mfcc_mean'] = float(np.mean(mfccs))
                features['mfcc_std'] = float(np.std(mfccs))
            except:
                features['mfcc_mean'] = 0.0
                features['mfcc_std'] = 0.0
            
            return features
            
        except Exception as e:
            self.logger.error(f"Feature extraction failed: {e}")
            return {}




class RiskAssessment:
    """Assess emotional risk levels for consent and training purposes"""
    
    def __init__(self, config: EmotionConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Risk scoring weights
        self.risk_weights = {
            'angry': 0.9,
            'frustrated': 0.7,
            'anxiety': 0.8,
            'stress': 0.8,
            'pain': 0.9,
            'fear': 0.6,
            'sad': 0.4,
            'neutral': 0.0,
            'calm': 0.0,
            'happy': 0.0,
            'excited': 0.1,
            'surprise': 0.2
        }
    
    def assess_risk(self, emotion_scores: Dict[str, float], 
                   acoustic_features: Dict[str, float] = None) -> Tuple[str, float, List[str]]:
        """
        Assess emotional risk level
        Returns: (risk_level, risk_score, risk_factors)
        """
        try:
            risk_factors = []
            risk_score = 0.0
            
            # Calculate emotion-based risk
            for emotion, confidence in emotion_scores.items():
                weight = self.risk_weights.get(emotion, 0.0)
                risk_score += confidence * weight
                
                if emotion in self.config.risk_emotions and confidence > self.config.high_risk_threshold:
                    risk_factors.append(f"high_{emotion}")
            
            # Add acoustic-based risk factors
            if acoustic_features:
                # High energy + high pitch might indicate stress/anger
                if (acoustic_features.get('rms_energy', 0) > 0.15 and 
                    acoustic_features.get('f0_mean', 0) > 250):
                    risk_factors.append("acoustic_stress_indicators")
                    risk_score += 0.2
                
                # Very low energy might indicate depression/sadness
                if acoustic_features.get('rms_energy', 0) < 0.05:
                    risk_factors.append("low_energy_depression")
                    risk_score += 0.1
                
                # High pitch variability might indicate anxiety
                if acoustic_features.get('f0_std', 0) > 60:
                    risk_factors.append("high_pitch_variability")
                    risk_score += 0.15
            
            # Determine risk level
            if risk_score >= 0.7:
                risk_level = "high"
            elif risk_score >= 0.4:
                risk_level = "medium"
            else:
                risk_level = "low"
            
            return risk_level, float(np.clip(risk_score, 0.0, 1.0)), risk_factors
            
        except Exception as e:
            self.logger.error(f"Risk assessment failed: {e}")
            return "medium", 0.5, ["assessment_error"]
    
    def should_trigger_consent_review(self, risk_level: str, risk_score: float, 
                                    emotion_scores: Dict[str, float]) -> bool:
        """Determine if emotional state should trigger consent review"""
        
        # High-risk emotions should trigger review
        if risk_level == "high":
            return True
        
        # Specific high-confidence negative emotions
        concerning_emotions = ["angry", "frustrated", "anxiety", "stress", "pain"]
        for emotion in concerning_emotions:
            if emotion_scores.get(emotion, 0) > 0.8:
                return True
        
        return False
    
    def get_training_metadata(self, prediction: EmotionPrediction) -> Dict[str, Any]:
        """Generate metadata for AI training systems"""
        return {
            'emotional_context': {
                'primary_emotion': prediction.primary_emotion,
                'confidence': prediction.primary_confidence,
                'risk_level': prediction.risk_level,
                'risk_score': prediction.risk_score,
                'emotional_state': 'stable' if prediction.risk_level == 'low' else 'elevated'
            },
            'consent_implications': {
                'requires_review': self.should_trigger_consent_review(
                    prediction.risk_level, 
                    prediction.risk_score, 
                    prediction.emotion_scores
                ),
                'risk_factors': prediction.risk_factors
            },
            'training_weights': {
                'emotional_reliability': 1.0 - (prediction.risk_score * 0.5),
                'consent_stability': 1.0 if prediction.risk_level == 'low' else 0.5,
                'data_quality_factor': prediction.primary_confidence
            }
        }




class EmotionBuffer:
    """Buffer for emotion detection with temporal smoothing"""
    
    def __init__(self, min_duration: float = 1.0, max_duration: float = 5.0, 
                 sample_rate: int = 16000, smoothing_window: int = 5):
        self.min_duration = min_duration
        self.max_duration = max_duration
        self.sample_rate = sample_rate
        self.smoothing_window = smoothing_window
        
        self.min_samples = int(min_duration * sample_rate)
        self.max_samples = int(max_duration * sample_rate)
        
        # Audio buffer
        self.audio_buffer = np.array([], dtype=np.float32)
        self.buffer_start_time = None
        
        # Prediction history for smoothing
        self.prediction_history = deque(maxlen=smoothing_window)
        
        self.lock = threading.Lock()
        self.logger = logging.getLogger(__name__)
    
    def add_audio(self, audio_data: np.ndarray, is_speech: bool, timestamp: float) -> bool:
        """
        Add audio data to buffer
        Returns True if buffer is ready for processing
        """
        with self.lock:
            if self.buffer_start_time is None:
                self.buffer_start_time = timestamp
            
            # Only add speech segments for emotion detection
            if is_speech:
                self.audio_buffer = np.concatenate([self.audio_buffer, audio_data])
            
            # Trim if too long
            if len(self.audio_buffer) > self.max_samples:
                excess = len(self.audio_buffer) - self.max_samples
                self.audio_buffer = self.audio_buffer[excess:]
            
            # Check if ready for processing
            return len(self.audio_buffer) >= self.min_samples
    
    def get_audio_for_processing(self) -> Tuple[np.ndarray, float]:
        """Get audio for emotion detection"""
        with self.lock:
            if len(self.audio_buffer) < self.min_samples:
                return np.array([]), 0.0
            
            audio_to_process = self.audio_buffer.copy()
            duration = len(audio_to_process) / self.sample_rate
            
            return audio_to_process, duration
    
    def add_prediction(self, prediction: EmotionPrediction):
        """Add prediction to history for smoothing"""
        with self.lock:
            self.prediction_history.append(prediction)
    
    def get_smoothed_emotion(self) -> Dict[str, float]:
        """Get smoothed emotion scores from recent predictions"""
        with self.lock:
            if not self.prediction_history:
                return {}
            
            # Average emotion scores over recent predictions
            emotion_sums = {}
            total_weight = 0.0
            
            for pred in self.prediction_history:
                weight = pred.primary_confidence  # Weight by confidence
                total_weight += weight
                
                for emotion, score in pred.emotion_scores.items():
                    emotion_sums[emotion] = emotion_sums.get(emotion, 0.0) + (score * weight)
            
            if total_weight > 0:
                smoothed_emotions = {
                    emotion: score / total_weight 
                    for emotion, score in emotion_sums.items()
                }
                return smoothed_emotions
            
            return {}
    
    def clear_buffer(self):
        """Clear the audio buffer"""
        with self.lock:
            self.audio_buffer = np.array([], dtype=np.float32)
            self.buffer_start_time = None
    
    def get_buffer_status(self) -> Dict[str, Any]:
        """Get current buffer status"""
        with self.lock:
            return {
                'buffer_samples': len(self.audio_buffer),
                'buffer_duration': len(self.audio_buffer) / self.sample_rate,
                'ready_for_processing': len(self.audio_buffer) >= self.min_samples,
                'prediction_history_length': len(self.prediction_history)
            }




class EmotionPlugin:
    """Main emotion detection plugin implementing PluginInterface"""
    
    def __init__(self, config: EmotionConfig, storage_plugin=None, consent_manager=None):
        self.config = config
        self.storage_plugin = storage_plugin
        self.consent_manager = consent_manager
        self._enabled = config.enabled
        
        # Core components
        self.emotion_detector = None
        self.risk_assessor = RiskAssessment(config)
        
        # Audio buffers per speaker
        self.emotion_buffers: Dict[str, EmotionBuffer] = {}
        
        # Current session tracking
        self.current_session: Optional[str] = None
        self.prediction_counter = 0
        self.session_emotion_timeline: List[EmotionPrediction] = []
        
        # Processing queues
        self.processing_queue = queue.Queue()
        self.processing_thread = None
        self.running = threading.Event()
        
        # Callbacks for other plugins
        self.callbacks: Dict[str, List] = {
            'emotion_detected': [],      # New emotion prediction
            'risk_alert': [],           # High-risk emotional state
            'consent_review_needed': [], # Emotion suggests consent review
            'training_metadata': []     # Metadata for training systems
        }
        
        self.logger = logging.getLogger(__name__)
        
        # Initialize emotion detector
        if self.enabled:
            self._initialize_detector()
    
    @property
    def name(self) -> str:
        return "EmotionPlugin"
    
    @property
    def enabled(self) -> bool:
        return self._enabled
    
    def _initialize_detector(self):
        """Initialize emotion detection model"""
        try:
            self.emotion_detector = TransformerEmotionDetector(
                self.config.model_name,
                self.config.device
            )
            
            if self.emotion_detector.classifier or self.emotion_detector.model:
                self.logger.info("Emotion detector initialized successfully")
            else:
                self.logger.warning("Emotion detector using fallback mode")
                
        except Exception as e:
            self.logger.error(f"Failed to initialize emotion detector: {e}")
            self._enabled = False
    
    def process_audio(self, chunk: AudioChunk) -> Optional[AudioChunk]:
        """Process audio chunk for emotion detection (transparent to other plugins)"""
        if not self.enabled or not self.current_session:
            return chunk
        
        try:
            # Only process speech from known speakers
            if not chunk.speaker_id or not chunk.is_speech:
                return chunk
            
            # Get or create emotion buffer for this speaker
            if chunk.speaker_id not in self.emotion_buffers:
                self.emotion_buffers[chunk.speaker_id] = EmotionBuffer(
                    self.config.min_audio_duration,
                    self.config.max_audio_duration,
                    chunk.sample_rate,
                    self.config.smoothing_window
                )
            
            buffer = self.emotion_buffers[chunk.speaker_id]
            
            # Add audio to buffer
            if buffer.add_audio(chunk.data, chunk.is_speech, chunk.timestamp):
                # Buffer is ready for processing
                self._queue_emotion_detection(chunk.speaker_id, chunk.session_id, 
                                            chunk.consent_token, chunk.voice_activity_score)
            
        except Exception as e:
            self.logger.error(f"Error processing audio chunk for emotion: {e}")
        
        # Return chunk unchanged (transparent operation)
        return chunk
    
    def on_recording_started(self, session_id: str) -> None:
        """Initialize emotion detection for new session"""
        try:
            self.logger.info(f"Starting emotion detection for session: {session_id}")
            
            self.current_session = session_id
            self.prediction_counter = 0
            self.session_emotion_timeline.clear()
            self.emotion_buffers.clear()
            
            # Start processing thread
            self.running.set()
            self.processing_thread = threading.Thread(
                target=self._emotion_worker,
                daemon=True,
                name="EmotionWorker"
            )
            self.processing_thread.start()
            
        except Exception as e:
            self.logger.error(f"Failed to start emotion detection: {e}")
    
    def on_recording_stopped(self, session_id: str) -> None:
        """Finalize emotion detection for completed session"""
        try:
            self.logger.info(f"Stopping emotion detection for session: {session_id}")
            
            # Process any remaining audio in buffers
            self._process_remaining_buffers()
            
            # Stop processing thread
            self.running.clear()
            if self.processing_thread and self.processing_thread.is_alive():
                self.processing_thread.join(timeout=3.0)
            
            # Save session emotion timeline
            if self.config.save_emotion_timeline:
                self._save_emotion_timeline()
            
            self.current_session = None
            
        except Exception as e:
            self.logger.error(f"Failed to stop emotion detection: {e}")
    
    def on_speaker_detected(self, speaker_id: str, session_id: str) -> None:
        """Handle new speaker detection"""
        if not self.enabled:
            return
        
        try:
            # Create emotion buffer for new speaker
            if speaker_id not in self.emotion_buffers:
                self.emotion_buffers[speaker_id] = EmotionBuffer(
                    self.config.min_audio_duration,
                    self.config.max_audio_duration,
                    16000,  # Default sample rate
                    self.config.smoothing_window
                )
            
            self.logger.info(f"Speaker {speaker_id} initialized for emotion detection")
            
        except Exception as e:
            self.logger.error(f"Error handling speaker detection: {e}")
    
    def _queue_emotion_detection(self, speaker_id: str, session_id: str, 
                               consent_token: str, voice_activity_score: float):
        """Queue audio for emotion detection"""
        try:
            self.processing_queue.put({
                'speaker_id': speaker_id,
                'session_id': session_id,
                'consent_token': consent_token,
                'voice_activity_score': voice_activity_score,
                'timestamp': time.time()
            }, block=False)
        except queue.Full:
            self.logger.warning("Emotion detection queue full, dropping audio")
    
    def _emotion_worker(self):
        """Background worker for emotion detection"""
        while self.running.is_set():
            try:
                # Get processing task
                try:
                    task = self.processing_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                speaker_id = task['speaker_id']
                
                # Get audio from buffer
                if speaker_id in self.emotion_buffers:
                    audio_data, duration = self.emotion_buffers[speaker_id].get_audio_for_processing()
                    
                    if len(audio_data) > 0 and duration >= self.config.min_audio_duration:
                        # Detect emotions
                        self._detect_emotions(
                            audio_data,
                            speaker_id,
                            task['session_id'],
                            task['consent_token'],
                            duration,
                            task['voice_activity_score']
                        )
                
                self.processing_queue.task_done()
                
            except Exception as e:
                self.logger.error(f"Emotion worker error: {e}")
                time.sleep(0.1)
    
    def _detect_emotions(self, audio_data: np.ndarray, speaker_id: str, 
                        session_id: str, consent_token: str, duration: float,
                        voice_activity_score: float):
        """Detect emotions from audio data"""
        try:
            # Extract emotion scores
            emotion_scores = self.emotion_detector.predict_emotion(audio_data)
            
            if not emotion_scores:
                return
            
            # Find primary emotion
            primary_emotion = max(emotion_scores.items(), key=lambda x: x[1])
            primary_emotion_name = primary_emotion[0]
            primary_confidence = primary_emotion[1]
            
            # Skip if confidence is too low
            if primary_confidence < self.config.confidence_threshold:
                return
            
            # Extract acoustic features
            acoustic_features = self.emotion_detector._extract_acoustic_features(audio_data, 16000)
            
            # Assess risk
            risk_level, risk_score, risk_factors = self.risk_assessor.assess_risk(
                emotion_scores, acoustic_features
            )
            
            # Create prediction object
            self.prediction_counter += 1
            prediction_id = f"{session_id}_{speaker_id}_{self.prediction_counter}_{int(time.time())}"
            
            prediction = EmotionPrediction(
                prediction_id=prediction_id,
                session_id=session_id,
                speaker_id=speaker_id,
                timestamp=time.time(),
                duration=duration,
                primary_emotion=primary_emotion_name,
                primary_confidence=primary_confidence,
                emotion_scores=emotion_scores,
                risk_level=risk_level,
                risk_score=risk_score,
                risk_factors=risk_factors,
                acoustic_features=acoustic_features,
                consent_token=consent_token,
                is_speech=True,
                voice_activity_score=voice_activity_score
            )
            
            # Add to timeline and buffer history
            self.session_emotion_timeline.append(prediction)
            self.emotion_buffers[speaker_id].add_prediction(prediction)
            
            # Store in database if available
            if self.storage_plugin:
                self._store_emotion_prediction(prediction)
            
            # Check for risk alerts and consent implications
            self._handle_emotion_implications(prediction)
            
            # Trigger callbacks
            self._trigger_callback('emotion_detected', prediction.to_dict())
            
            self.logger.debug(f"Detected emotion for {speaker_id}: {primary_emotion_name} "
                            f"(confidence: {primary_confidence:.3f}, risk: {risk_level})")
            
        except Exception as e:
            self.logger.error(f"Error detecting emotions for speaker {speaker_id}: {e}")
    
    def _handle_emotion_implications(self, prediction: EmotionPrediction):
        """Handle implications of emotion detection for consent and training"""
        try:
            # Check if consent review is needed
            if self.risk_assessor.should_trigger_consent_review(
                prediction.risk_level, 
                prediction.risk_score, 
                prediction.emotion_scores
            ):
                self._trigger_callback('consent_review_needed', {
                    'speaker_id': prediction.speaker_id,
                    'prediction_id': prediction.prediction_id,
                    'risk_level': prediction.risk_level,
                    'primary_emotion': prediction.primary_emotion,
                    'risk_factors': prediction.risk_factors
                })
                
                # Notify consent manager if available
                if self.consent_manager:
                    try:
                        # This would trigger a consent review workflow
                        self.logger.warning(f"Emotional state of speaker {prediction.speaker_id} "
                                          f"suggests consent review needed: {prediction.primary_emotion}")
                    except Exception as e:
                        self.logger.error(f"Error notifying consent manager: {e}")
            
            # Trigger risk alert for high-risk emotions
            if prediction.risk_level == "high":
                self._trigger_callback('risk_alert', {
                    'speaker_id': prediction.speaker_id,
                    'prediction_id': prediction.prediction_id,
                    'risk_score': prediction.risk_score,
                    'primary_emotion': prediction.primary_emotion,
                    'risk_factors': prediction.risk_factors,
                    'timestamp': prediction.timestamp
                })
            
            # Generate training metadata
            training_metadata = self.risk_assessor.get_training_metadata(prediction)
            self._trigger_callback('training_metadata', {
                'speaker_id': prediction.speaker_id,
                'prediction_id': prediction.prediction_id,
                'metadata': training_metadata
            })
            
        except Exception as e:
            self.logger.error(f"Error handling emotion implications: {e}")
    
    def _store_emotion_prediction(self, prediction: EmotionPrediction):
        """Store emotion prediction in database"""
        try:
            if not self.storage_plugin:
                return
            
            # Store in emotion-specific table (extend storage schema)
            self.storage_plugin.db_manager.execute_update('''
                INSERT OR IGNORE INTO emotion_predictions 
                (prediction_id, session_id, speaker_id, timestamp, duration,
                 primary_emotion, primary_confidence, emotion_scores_json,
                 risk_level, risk_score, risk_factors_json, acoustic_features_json,
                 consent_token, voice_activity_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                prediction.prediction_id,
                prediction.session_id,
                prediction.speaker_id,
                prediction.timestamp,
                prediction.duration,
                prediction.primary_emotion,
                prediction.primary_confidence,
                json.dumps(prediction.emotion_scores),
                prediction.risk_level,
                prediction.risk_score,
                json.dumps(prediction.risk_factors),
                json.dumps(prediction.acoustic_features),
                prediction.consent_token,
                prediction.voice_activity_score
            ))
            
        except Exception as e:
            # If table doesn't exist, we'll create it dynamically
            if "no such table" in str(e).lower():
                self._create_emotion_table()
                # Retry storage
                self._store_emotion_prediction(prediction)
            else:
                self.logger.error(f"Error storing emotion prediction: {e}")
    
    def _create_emotion_table(self):
        """Create emotion predictions table in database"""
        try:
            if not self.storage_plugin:
                return
            
            self.storage_plugin.db_manager.execute_update('''
                CREATE TABLE IF NOT EXISTS emotion_predictions (
                    prediction_id TEXT PRIMARY KEY,
                    session_id TEXT NOT NULL,
                    speaker_id TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    duration REAL,
                    primary_emotion TEXT,
                    primary_confidence REAL,
                    emotion_scores_json TEXT,
                    risk_level TEXT,
                    risk_score REAL,
                    risk_factors_json TEXT,
                    acoustic_features_json TEXT,
                    consent_token TEXT,
                    voice_activity_score REAL,
                    created_at REAL DEFAULT (strftime('%s', 'now')),
                    FOREIGN KEY (session_id) REFERENCES sessions (session_id),
                    FOREIGN KEY (speaker_id) REFERENCES speakers (speaker_id)
                )
            ''')
            
            # Create indexes
            self.storage_plugin.db_manager.execute_update(
                'CREATE INDEX IF NOT EXISTS idx_emotion_session ON emotion_predictions (session_id)'
            )
            self.storage_plugin.db_manager.execute_update(
                'CREATE INDEX IF NOT EXISTS idx_emotion_speaker ON emotion_predictions (speaker_id)'
            )
            self.storage_plugin.db_manager.execute_update(
                'CREATE INDEX IF NOT EXISTS idx_emotion_timestamp ON emotion_predictions (timestamp)'
            )
            
            self.logger.info("Created emotion predictions table")
            
        except Exception as e:
            self.logger.error(f"Error creating emotion table: {e}")
    
    def _process_remaining_buffers(self):
        """Process any remaining audio in buffers before shutdown"""
        try:
            for speaker_id, buffer in self.emotion_buffers.items():
                audio_data, duration = buffer.get_audio_for_processing()
                if len(audio_data) > 0:
                    self._detect_emotions(
                        audio_data,
                        speaker_id,
                        self.current_session or "session_end",
                        "session_end",
                        duration,
                        0.0
                    )
        except Exception as e:
            self.logger.error(f"Error processing remaining buffers: {e}")
    
    def _save_emotion_timeline(self):
        """Save session emotion timeline to file"""
        try:
            if not self.config.save_emotion_timeline or not self.session_emotion_timeline:
                return
            
            # Create emotion storage directory
            emotion_dir = Path(self.config.emotion_storage_path)
            emotion_dir.mkdir(parents=True, exist_ok=True)
            
            # Save timeline as JSON
            timeline_file = emotion_dir / f"emotion_timeline_{self.current_session}.json"
            
            timeline_data = {
                'session_id': self.current_session,
                'total_predictions': len(self.session_emotion_timeline),
                'session_duration': max(p.timestamp for p in self.session_emotion_timeline) - 
                                  min(p.timestamp for p in self.session_emotion_timeline),
                'speakers': list(set(p.speaker_id for p in self.session_emotion_timeline)),
                'emotion_timeline': [pred.to_dict() for pred in self.session_emotion_timeline],
                'session_summary': self._generate_session_emotion_summary()
            }
            
            with open(timeline_file, 'w') as f:
                json.dump(timeline_data, f, indent=2, default=str)
            
            self.logger.info(f"Saved emotion timeline: {timeline_file}")
            
        except Exception as e:
            self.logger.error(f"Error saving emotion timeline: {e}")
    
    def _generate_session_emotion_summary(self) -> Dict[str, Any]:
        """Generate summary of emotions detected in session"""
        try:
            if not self.session_emotion_timeline:
                return {}
            
            # Emotion distribution
            emotion_counts = {}
            risk_counts = {"low": 0, "medium": 0, "high": 0}
            total_risk_score = 0.0
            
            for pred in self.session_emotion_timeline:
                emotion_counts[pred.primary_emotion] = emotion_counts.get(pred.primary_emotion, 0) + 1
                risk_counts[pred.risk_level] += 1
                total_risk_score += pred.risk_score
            
            # Speaker-specific summaries
            speaker_summaries = {}
            for pred in self.session_emotion_timeline:
                if pred.speaker_id not in speaker_summaries:
                    speaker_summaries[pred.speaker_id] = {
                        'prediction_count': 0,
                        'dominant_emotion': None,
                        'avg_risk_score': 0.0,
                        'high_risk_count': 0
                    }
                
                speaker_summaries[pred.speaker_id]['prediction_count'] += 1
                speaker_summaries[pred.speaker_id]['avg_risk_score'] += pred.risk_score
                if pred.risk_level == "high":
                    speaker_summaries[pred.speaker_id]['high_risk_count'] += 1
            
            # Calculate averages
            for speaker_id, summary in speaker_summaries.items():
                summary['avg_risk_score'] /= summary['prediction_count']
                
                # Find dominant emotion for speaker
                speaker_emotions = [p.primary_emotion for p in self.session_emotion_timeline 
                                  if p.speaker_id == speaker_id]
                summary['dominant_emotion'] = max(set(speaker_emotions), key=speaker_emotions.count)
            
            return {
                'total_predictions': len(self.session_emotion_timeline),
                'emotion_distribution': emotion_counts,
                'risk_distribution': risk_counts,
                'average_risk_score': total_risk_score / len(self.session_emotion_timeline),
                'speaker_summaries': speaker_summaries,
                'high_risk_moments': len([p for p in self.session_emotion_timeline if p.risk_level == "high"]),
                'consent_review_triggers': len([p for p in self.session_emotion_timeline 
                                              if self.risk_assessor.should_trigger_consent_review(
                                                  p.risk_level, p.risk_score, p.emotion_scores)])
            }
            
        except Exception as e:
            self.logger.error(f"Error generating session summary: {e}")
            return {}
    
    # Public API methods for other plugins
    
    def get_speaker_emotion_state(self, speaker_id: str) -> Dict[str, Any]:
        """Get current emotional state for a speaker"""
        try:
            if speaker_id not in self.emotion_buffers:
                return {}
            
            buffer = self.emotion_buffers[speaker_id]
            smoothed_emotions = buffer.get_smoothed_emotion()
            
            if not smoothed_emotions:
                return {}
            
            # Get most recent prediction
            recent_predictions = [p for p in self.session_emotion_timeline 
                                if p.speaker_id == speaker_id]
            if not recent_predictions:
                return {}
            
            latest_prediction = recent_predictions[-1]
            
            return {
                'speaker_id': speaker_id,
                'current_emotions': smoothed_emotions,
                'primary_emotion': latest_prediction.primary_emotion,
                'confidence': latest_prediction.primary_confidence,
                'risk_level': latest_prediction.risk_level,
                'risk_score': latest_prediction.risk_score,
                'last_update': latest_prediction.timestamp,
                'buffer_status': buffer.get_buffer_status()
            }
            
        except Exception as e:
            self.logger.error(f"Error getting speaker emotion state: {e}")
            return {}
    
    def get_session_emotion_overview(self) -> Dict[str, Any]:
        """Get overview of emotions in current session"""
        try:
            if not self.session_emotion_timeline:
                return {}
            
            # Recent activity (last 5 minutes)
            recent_time = time.time() - 300
            recent_predictions = [p for p in self.session_emotion_timeline 
                                if p.timestamp > recent_time]
            
            # Current speaker states
            speaker_states = {}
            for speaker_id in self.emotion_buffers.keys():
                speaker_states[speaker_id] = self.get_speaker_emotion_state(speaker_id)
            
            return {
                'session_id': self.current_session,
                'total_predictions': len(self.session_emotion_timeline),
                'recent_predictions': len(recent_predictions),
                'active_speakers': len(self.emotion_buffers),
                'speaker_states': speaker_states,
                'processing_queue_size': self.processing_queue.qsize(),
                'session_summary': self._generate_session_emotion_summary()
            }
            
        except Exception as e:
            self.logger.error(f"Error getting session emotion overview: {e}")
            return {}
    
    def get_training_emotion_metadata(self, speaker_id: str, timestamp_range: Tuple[float, float] = None) -> List[Dict[str, Any]]:
        """Get emotion metadata for training systems"""
        try:
            predictions = self.session_emotion_timeline
            
            # Filter by speaker
            if speaker_id:
                predictions = [p for p in predictions if p.speaker_id == speaker_id]
            
            # Filter by timestamp range
            if timestamp_range:
                start_time, end_time = timestamp_range
                predictions = [p for p in predictions 
                             if start_time <= p.timestamp <= end_time]
            
            # Generate training metadata for each prediction
            training_metadata = []
            for pred in predictions:
                metadata = self.risk_assessor.get_training_metadata(pred)
                metadata['prediction_data'] = pred.to_dict()
                training_metadata.append(metadata)
            
            return training_metadata
            
        except Exception as e:
            self.logger.error(f"Error getting training emotion metadata: {e}")
            return []
    
    def update_risk_thresholds(self, high_risk_threshold: float = None, 
                             emotion_weights: Dict[str, float] = None):
        """Update risk assessment parameters"""
        try:
            if high_risk_threshold is not None:
                self.config.high_risk_threshold = high_risk_threshold
            
            if emotion_weights:
                self.risk_assessor.risk_weights.update(emotion_weights)
            
            self.logger.info("Updated emotion risk assessment parameters")
            
        except Exception as e:
            self.logger.error(f"Error updating risk thresholds: {e}")
    
    def add_callback(self, event_type: str, callback):
        """Add callback for emotion events"""
        if event_type in self.callbacks:
            self.callbacks[event_type].append(callback)
    
    def _trigger_callback(self, event_type: str, data: Any):
        """Trigger callbacks for emotion events"""
        for callback in self.callbacks.get(event_type, []):
            try:
                callback(data)
            except Exception as e:
                self.logger.error(f"Callback error for {event_type}: {e}")
    
    def cleanup(self) -> None:
        """Clean up emotion plugin resources"""
        self.running.clear()
        
        if self.processing_thread and self.processing_thread.is_alive():
            self.processing_thread.join(timeout=3.0)
        
        # Clear buffers
        for buffer in self.emotion_buffers.values():
            buffer.clear_buffer()
        
        self.emotion_buffers.clear()
        self.session_emotion_timeline.clear()
        
        self.logger.info("Emotion plugin cleaned up")




# Factory and utility functions


def create_emotion_plugin(config_path: str = "config/plugins_config.yaml",
                         storage_plugin=None, consent_manager=None) -> EmotionPlugin:
    """Create emotion plugin with configuration"""
    try:
        import yaml
        with open(config_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        
        emotion_dict = config_dict.get('plugins', {}).get('emotion', {})
        config = EmotionConfig(**emotion_dict)
        
        return EmotionPlugin(config, storage_plugin, consent_manager)
        
    except Exception as e:
        logging.warning(f"Error loading emotion config: {e}, using defaults")
        return EmotionPlugin(EmotionConfig(), storage_plugin, consent_manager)




def test_emotion_detection(audio_file_path: str, model_name: str = None) -> Dict[str, Any]:
    """Test emotion detection on audio file"""
    try:
        import librosa
        
        # Load audio file
        audio_data, sample_rate = librosa.load(audio_file_path, sr=16000)
        
        # Initialize detector
        detector = TransformerEmotionDetector(
            model_name or "facebook/wav2vec2-large-xlsr-53-emotional-speech",
            "cuda" if torch.cuda.is_available() else "cpu"
        )
        
        # Detect emotions
        emotion_scores = detector.predict_emotion(audio_data, sample_rate)
        
        if not emotion_scores:
            return {'success': False, 'error': 'No emotions detected'}
        
        # Find primary emotion
        primary_emotion = max(emotion_scores.items(), key=lambda x: x[1])
        
        # Extract acoustic features
        acoustic_features = detector._extract_acoustic_features(audio_data, sample_rate)
        
        # Assess risk
        risk_assessor = RiskAssessment(EmotionConfig())
        risk_level, risk_score, risk_factors = risk_assessor.assess_risk(emotion_scores, acoustic_features)
        
        return {
            'success': True,
            'file_path': audio_file_path,
            'audio_duration': len(audio_data) / sample_rate,
            'primary_emotion': primary_emotion[0],
            'primary_confidence': primary_emotion[1],
            'all_emotions': emotion_scores,
            'risk_assessment': {
                'risk_level': risk_level,
                'risk_score': risk_score,
                'risk_factors': risk_factors
            },
            'acoustic_features': acoustic_features
        }
        
    except Exception as e:
        return {'success': False, 'error': str(e)}




class EmotionTestSuite:
    """Comprehensive testing suite for emotion plugin"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def test_model_availability(self) -> Dict[str, Any]:
        """Test if emotion detection models are available"""
        try:
            results = {
                'transformers_available': TRANSFORMERS_AVAILABLE,
                'opensmile_available': OPENSMILE_AVAILABLE
            }
            
            if TRANSFORMERS_AVAILABLE:
                # Test model loading
                detector = TransformerEmotionDetector(
                    "facebook/wav2vec2-base-960h",  # Use smaller model for testing
                    "cpu"
                )
                
                results['model_loaded'] = detector.classifier is not None or detector.model is not None
                
                if results['model_loaded']:
                    # Test prediction
                    test_audio = np.random.random(16000).astype(np.float32)
                    emotions = detector.predict_emotion(test_audio)
                    results['prediction_test'] = len(emotions) > 0
                    results['detected_emotions'] = list(emotions.keys())
            else:
                results['model_loaded'] = False
                results['prediction_test'] = False
            
            return {
                'success': True,
                'results': results
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def test_emotion_buffer(self) -> Dict[str, Any]:
        """Test emotion buffering functionality"""
        try:
            buffer = EmotionBuffer(min_duration=1.0, max_duration=3.0, sample_rate=16000)
            
            # Test adding audio chunks
            chunk1 = np.random.random(8000).astype(np.float32)   # 0.5 seconds
            chunk2 = np.random.random(8000).astype(np.float32)   # 0.5 seconds
            chunk3 = np.random.random(4000).astype(np.float32)   # 0.25 seconds
            
            ready1 = buffer.add_audio(chunk1, True, time.time())
            ready2 = buffer.add_audio(chunk2, True, time.time() + 0.5)
            ready3 = buffer.add_audio(chunk3, True, time.time() + 1.0)
            
            # Should be ready after chunk2 (1.0 seconds total)
            if ready2 or ready3:
                audio_data, duration = buffer.get_audio_for_processing()
                
                return {
                    'success': True,
                    'buffer_tests': {
                        'ready_after_chunk1': ready1,
                        'ready_after_chunk2': ready2,
                        'ready_after_chunk3': ready3,
                        'extracted_duration': duration,
                        'extracted_samples': len(audio_data)
                    },
                    'buffer_status': buffer.get_buffer_status()
                }
            else:
                return {'success': False, 'error': 'Buffer not ready when expected'}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def test_risk_assessment(self) -> Dict[str, Any]:
        """Test risk assessment functionality"""
        try:
            risk_assessor = RiskAssessment(EmotionConfig())
            
            # Test different emotion scenarios
            test_cases = [
                # Low risk scenario
                {'neutral': 0.8, 'calm': 0.2},
                # Medium risk scenario
                {'sad': 0.6, 'neutral': 0.4},
                # High risk scenario
                {'angry': 0.9, 'frustrated': 0.1},
                # Mixed emotions
                {'happy': 0.4, 'anxiety': 0.6}
            ]
            
            results = []
            for i, emotions in enumerate(test_cases):
                risk_level, risk_score, risk_factors = risk_assessor.assess_risk(emotions)
                
                results.append({
                    'test_case': i + 1,
                    'input_emotions': emotions,
                    'risk_level': risk_level,
                    'risk_score': risk_score,
                    'risk_factors': risk_factors,
                    'consent_review_needed': risk_assessor.should_trigger_consent_review(
                        risk_level, risk_score, emotions
                    )
                })
            
            return {
                'success': True,
                'test_results': results
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def test_acoustic_features(self) -> Dict[str, Any]:
        """Test acoustic feature extraction"""
        try:
            detector = TransformerEmotionDetector("facebook/wav2vec2-base-960h", "cpu")
            
            # Generate test audio with different characteristics
            sample_rate = 16000
            duration = 2.0
            t = np.linspace(0, duration, int(sample_rate * duration))
            
            test_cases = [
                # High energy signal
                np.sin(2 * np.pi * 440 * t) * 0.5,
                # Low energy signal  
                np.sin(2 * np.pi * 220 * t) * 0.1,
                # Noisy signal
                np.random.random(len(t)) * 0.3,
                # Mixed frequency signal
                np.sin(2 * np.pi * 440 * t) * 0.3 + np.sin(2 * np.pi * 880 * t) * 0.2
            ]
            
            results = []
            for i, audio in enumerate(test_cases):
                features = detector._extract_acoustic_features(audio.astype(np.float32), sample_rate)
                results.append({
                    'test_case': i + 1,
                    'features': features,
                    'feature_count': len(features)
                })
            
            return {
                'success': True,
                'acoustic_tests': results
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Run all emotion detection tests"""
        self.logger.info("Running emotion plugin test suite...")
        
        results = {
            'model_availability': self.test_model_availability(),
            'emotion_buffer': self.test_emotion_buffer(),
            'risk_assessment': self.test_risk_assessment(),
            'acoustic_features': self.test_acoustic_features(),
            'test_timestamp': time.time()
        }
        
        # Calculate overall success
        all_successful = all(
            result.get('success', False) for result in results.values() 
            if isinstance(result, dict) and 'success' in result
        )
        
        results['overall_success'] = all_successful
        results['summary'] = {
            'models_available': results['model_availability']['success'],
            'buffer_working': results['emotion_buffer']['success'],
            'risk_assessment_working': results['risk_assessment']['success'],
            'acoustic_features_working': results['acoustic_features']['success']
        }
        
        self.logger.info(f"Test suite completed. Overall success: {all_successful}")
        return results




# Integration utilities for Eden system


class EmotionIntegrationManager:
    """Manager for integrating emotion detection with other Eden plugins"""
    
    def __init__(self, emotion_plugin: EmotionPlugin, consent_manager=None, 
                 transcription_plugin=None, storage_plugin=None):
        self.emotion_plugin = emotion_plugin
        self.consent_manager = consent_manager
        self.transcription_plugin = transcription_plugin
        self.storage_plugin = storage_plugin
        
        self.logger = logging.getLogger(__name__)
        
        # Set up cross-plugin callbacks
        self._setup_integrations()
    
    def _setup_integrations(self):
        """Set up integration callbacks between plugins"""
        try:
            # Emotion -> Consent integration
            if self.consent_manager:
                self.emotion_plugin.add_callback('risk_alert', self._handle_emotion_risk_alert)
                self.emotion_plugin.add_callback('consent_review_needed', self._handle_consent_review)
            
            # Emotion -> Transcription integration
            if self.transcription_plugin:
                self.emotion_plugin.add_callback('emotion_detected', self._handle_emotion_for_transcription)
            
            # Training metadata integration
            self.emotion_plugin.add_callback('training_metadata', self._handle_training_metadata)
            
            self.logger.info("Emotion plugin integrations set up successfully")
            
        except Exception as e:
            self.logger.error(f"Error setting up emotion integrations: {e}")
    
    def _handle_emotion_risk_alert(self, risk_data: Dict[str, Any]):
        """Handle high-risk emotional states"""
        try:
            speaker_id = risk_data['speaker_id']
            risk_score = risk_data['risk_score']
            primary_emotion = risk_data['primary_emotion']
            
            self.logger.warning(f"High emotional risk detected for speaker {speaker_id}: "
                              f"{primary_emotion} (risk score: {risk_score:.3f})")
            
            # Could trigger additional monitoring or safety measures
            
        except Exception as e:
            self.logger.error(f"Error handling emotion risk alert: {e}")
    
    def _handle_consent_review(self, review_data: Dict[str, Any]):
        """Handle consent review triggers from emotion detection"""
        try:
            speaker_id = review_data['speaker_id']
            risk_level = review_data['risk_level']
            primary_emotion = review_data['primary_emotion']
            
            self.logger.info(f"Emotion-triggered consent review for speaker {speaker_id}: "
                           f"{primary_emotion} (risk: {risk_level})")
            
            # Here you could implement automatic consent review workflows
            # For now, we'll just log the event
            
        except Exception as e:
            self.logger.error(f"Error handling consent review: {e}")
    
    def _handle_emotion_for_transcription(self, emotion_data: Dict[str, Any]):
        """Provide emotion context to transcription system"""
        try:
            if not self.transcription_plugin:
                return
            
            # Add emotion metadata to transcription context
            # This could influence PHI detection sensitivity or redaction timing
            speaker_id = emotion_data['speaker_id']
            primary_emotion = emotion_data['primary_emotion']
            risk_level = emotion_data['risk_level']
            
            # High-risk emotions might warrant more aggressive PHI protection
            if risk_level == "high":
                self.logger.info(f"High emotional risk for speaker {speaker_id} - "
                                f"consider enhanced PHI protection")
            
        except Exception as e:
            self.logger.error(f"Error handling emotion for transcription: {e}")
    
    def _handle_training_metadata(self, training_data: Dict[str, Any]):
        """Handle training metadata from emotion detection"""
        try:
            metadata = training_data['metadata']
            speaker_id = training_data['speaker_id']
            
            # Store training metadata for future AI model training
            if self.storage_plugin:
                self.storage_plugin.log_plugin_metric(
                    "EmotionPlugin",
                    "training_metadata",
                    metadata['training_weights']['emotional_reliability'],
                    metadata=metadata
                )
            
            self.logger.debug(f"Stored emotion training metadata for speaker {speaker_id}")
            
        except Exception as e:
            self.logger.error(f"Error handling training metadata: {e}")
    
    def get_speaker_emotion_context(self, speaker_id: str, 
                                  time_window: float = 300) -> Dict[str, Any]:
        """Get emotion context for a speaker over a time window"""
        try:
            current_time = time.time()
            start_time = current_time - time_window
            
            # Get recent emotion predictions
            recent_emotions = []
            for pred in self.emotion_plugin.session_emotion_timeline:
                if (pred.speaker_id == speaker_id and 
                    pred.timestamp >= start_time):
                    recent_emotions.append(pred)
            
            if not recent_emotions:
                return {}
            
            # Calculate emotion trends
            emotion_counts = {}
            risk_trend = []
            
            for pred in recent_emotions:
                emotion_counts[pred.primary_emotion] = emotion_counts.get(pred.primary_emotion, 0) + 1
                risk_trend.append(pred.risk_score)
            
            # Determine dominant emotion and risk trend
            dominant_emotion = max(emotion_counts.items(), key=lambda x: x[1])[0] if emotion_counts else "neutral"
            avg_risk = np.mean(risk_trend) if risk_trend else 0.0
            risk_increasing = len(risk_trend) > 1 and risk_trend[-1] > risk_trend[0]
            
            return {
                'speaker_id': speaker_id,
                'time_window_minutes': time_window / 60,
                'total_predictions': len(recent_emotions),
                'dominant_emotion': dominant_emotion,
                'emotion_distribution': emotion_counts,
                'average_risk_score': avg_risk,
                'risk_increasing': risk_increasing,
                'latest_emotion': recent_emotions[-1].primary_emotion if recent_emotions else None,
                'latest_risk_level': recent_emotions[-1].risk_level if recent_emotions else None
            }
            
        except Exception as e:
            self.logger.error(f"Error getting emotion context: {e}")
            return {}




# Configuration template and example usage


EMOTION_CONFIG_TEMPLATE = """
# Emotion Detection Plugin Configuration
plugins:
  emotion:
    enabled: true
    model_name: "facebook/wav2vec2-large-xlsr-53-emotional-speech"
    device: "cuda"  # or "cpu"
    
    # Detection settings
    min_audio_duration: 1.0
    max_audio_duration: 5.0
    confidence_threshold: 0.6
    
    # Emotion categories (customizable)
    emotion_categories:
      - "neutral"
      - "calm" 
      - "happy"
      - "excited"
      - "angry"
      - "frustrated"
      - "sad"
      - "fear"
      - "anxiety"
      - "stress"
      - "pain"
      - "surprise"
    
    # Risk assessment
    risk_emotions:
      - "angry"
      - "frustrated"
      - "anxiety"
      - "stress"
      - "pain"
      - "fear"
    high_risk_threshold: 0.8
    
    # Processing settings
    buffer_overlap: 0.5
    smoothing_window: 5
    
    # Output settings
    save_emotion_timeline: true
    emotion_storage_path: "data/emotions"
"""




def create_complete_eden_system_with_emotion():
    """Example of creating complete Eden system with emotion detection"""
    from src.core.audio_recorder import create_core_audio_recorder, ConfigManager
    from src.plugins.storage.plugin import create_storage_plugin
    from src.plugins.consent.manager import create_consent_manager
    from src.plugins.encryption.plugin import create_encryption_plugin
    from src.plugins.transcription.plugin import create_transcription_plugin
    from src.plugins.voiceprint.plugin import create_voiceprint_plugin
    
    # Create config manager
    config_manager = ConfigManager()
    
    # Create core audio recorder
    audio_recorder = create_core_audio_recorder(config_manager)
    
    # Create plugins in dependency order
    storage_plugin = create_storage_plugin()
    consent_manager = create_consent_manager()
    encryption_plugin = create_encryption_plugin(consent_manager=consent_manager)
    transcription_plugin = create_transcription_plugin(storage_plugin=storage_plugin)
    voiceprint_plugin = create_voiceprint_plugin(storage_plugin=storage_plugin)
    
    # Create emotion plugin
    emotion_plugin = create_emotion_plugin(
        storage_plugin=storage_plugin, 
        consent_manager=consent_manager
    )
    
    # Register plugins with audio recorder
    audio_recorder.register_plugin(storage_plugin)
    audio_recorder.register_plugin(consent_manager)
    audio_recorder.register_plugin(encryption_plugin)
    audio_recorder.register_plugin(transcription_plugin)
    audio_recorder.register_plugin(voiceprint_plugin)
    audio_recorder.register_plugin(emotion_plugin)
    
    # Create integration manager
    emotion_integration = EmotionIntegrationManager(
        emotion_plugin,
        consent_manager,
        transcription_plugin,
        storage_plugin
    )
    
    return {
        'audio_recorder': audio_recorder,
        'storage_plugin': storage_plugin,
        'consent_manager': consent_manager,
        'encryption_plugin': encryption_plugin,
        'transcription_plugin': transcription_plugin,
        'voiceprint_plugin': voiceprint_plugin,
        'emotion_plugin': emotion_plugin,
        'emotion_integration': emotion_integration
    }




class EmotionAPI:
    """API interface for emotion detection and analysis"""
    
    def __init__(self, emotion_plugin: EmotionPlugin):
        self.emotion_plugin = emotion_plugin
        self.logger = logging.getLogger(__name__)
    
    def get_real_time_emotions(self, speaker_id: str = None) -> Dict[str, Any]:
        """Get real-time emotion states for all or specific speaker"""
        try:
            if speaker_id:
                return self.emotion_plugin.get_speaker_emotion_state(speaker_id)
            else:
                return self.emotion_plugin.get_session_emotion_overview()
                
        except Exception as e:
            self.logger.error(f"Error getting real-time emotions: {e}")
            return {}
    
    def analyze_emotional_risk(self, speaker_id: str, 
                              time_window: float = 300) -> Dict[str, Any]:
        """Analyze emotional risk for a speaker over time window"""
        try:
            # Get recent emotion predictions
            current_time = time.time()
            start_time = current_time - time_window
            
            recent_predictions = [
                p for p in self.emotion_plugin.session_emotion_timeline
                if p.speaker_id == speaker_id and p.timestamp >= start_time
            ]
            
            if not recent_predictions:
                return {'error': 'No recent emotion data for speaker'}
            
            # Analyze risk trends
            risk_scores = [p.risk_score for p in recent_predictions]
            high_risk_count = len([p for p in recent_predictions if p.risk_level == "high"])
            
            # Risk assessment
            avg_risk = np.mean(risk_scores)
            max_risk = max(risk_scores)
            risk_trend = "increasing" if len(risk_scores) > 1 and risk_scores[-1] > risk_scores[0] else "stable"
            
            # Emotion patterns
            emotion_sequence = [p.primary_emotion for p in recent_predictions]
            emotion_changes = len(set(emotion_sequence))
            
            return {
                'speaker_id': speaker_id,
                'analysis_window_minutes': time_window / 60,
                'risk_analysis': {
                    'average_risk_score': avg_risk,
                    'maximum_risk_score': max_risk,
                    'high_risk_moments': high_risk_count,
                    'risk_trend': risk_trend
                },
                'emotion_analysis': {
                    'total_predictions': len(recent_predictions),
                    'emotion_changes': emotion_changes,
                    'emotion_sequence': emotion_sequence,
                    'dominant_emotion': max(set(emotion_sequence), key=emotion_sequence.count)
                },
                'recommendations': self._generate_risk_recommendations(avg_risk, high_risk_count, emotion_changes)
            }
            
        except Exception as e:
            self.logger.error(f"Error analyzing emotional risk: {e}")
            return {'error': str(e)}
    
    def _generate_risk_recommendations(self, avg_risk: float, 
                                     high_risk_count: int, 
                                     emotion_changes: int) -> List[str]:
        """Generate recommendations based on emotional risk analysis"""
        recommendations = []
        
        if avg_risk > 0.7:
            recommendations.append("High average emotional risk detected - consider implementing additional consent safeguards")
        
        if high_risk_count > 3:
            recommendations.append("Multiple high-risk emotional moments - review consent validity and consider session pause")
        
        if emotion_changes > 5:
            recommendations.append("High emotional volatility detected - enhance monitoring and support measures")
        
        if avg_risk < 0.3 and high_risk_count == 0:
            recommendations.append("Stable emotional state - normal processing can continue")
        
        return recommendations
    
    def export_emotion_data(self, speaker_id: str = None, 
                           session_id: str = None,
                           format: str = "json") -> Dict[str, Any]:
        """Export emotion data for analysis or compliance"""
        try:
            predictions = self.emotion_plugin.session_emotion_timeline
            
            # Filter by speaker if specified
            if speaker_id:
                predictions = [p for p in predictions if p.speaker_id == speaker_id]
            
            # Filter by session if specified
            if session_id:
                predictions = [p for p in predictions if p.session_id == session_id]
            
            # Prepare export data
            export_data = {
                'export_info': {
                    'timestamp': datetime.now().isoformat(),
                    'speaker_id': speaker_id,
                    'session_id': session_id,
                    'total_predictions': len(predictions),
                    'format': format
                },
                'emotion_timeline': [pred.to_dict() for pred in predictions],
                'summary_statistics': self._calculate_export_statistics(predictions)
            }
            
            return {
                'success': True,
                'data': export_data,
                'total_records': len(predictions)
            }
            
        except Exception as e:
            self.logger.error(f"Error exporting emotion data: {e}")
            return {'success': False, 'error': str(e)}
    
    def _calculate_export_statistics(self, predictions: List[EmotionPrediction]) -> Dict[str, Any]:
        """Calculate summary statistics for emotion export"""
        if not predictions:
            return {}
        
        # Emotion distribution
        emotions = [p.primary_emotion for p in predictions]
        emotion_dist = {emotion: emotions.count(emotion) for emotion in set(emotions)}
        
        # Risk statistics
        risk_scores = [p.risk_score for p in predictions]
        risk_levels = [p.risk_level for p in predictions]
        
        # Temporal analysis
        timestamps = [p.timestamp for p in predictions]
        duration = max(timestamps) - min(timestamps)
        
        return {
            'emotion_distribution': emotion_dist,
            'risk_statistics': {
                'average_risk_score': np.mean(risk_scores),
                'max_risk_score': max(risk_scores),
                'min_risk_score': min(risk_scores),
                'high_risk_count': risk_levels.count('high'),
                'medium_risk_count': risk_levels.count('medium'),
                'low_risk_count': risk_levels.count('low')
            },
            'temporal_statistics': {
                'session_duration_minutes': duration / 60,
                'predictions_per_minute': len(predictions) / (duration / 60) if duration > 0 else 0,
                'start_time': datetime.fromtimestamp(min(timestamps)).isoformat(),
                'end_time': datetime.fromtimestamp(max(timestamps)).isoformat()
            }
        }




if __name__ == "__main__":
    # Test the emotion plugin
    logging.basicConfig(level=logging.INFO)
    
    print("=== Eden Emotion Detection Plugin Test Suite ===")
    
    # Run comprehensive tests
    test_suite = EmotionTestSuite()
    results = test_suite.run_all_tests()
    
    print(f"\n=== Test Results ===")
    print(f"Overall Success: {results['overall_success']}")
    print(f"Models Available: {results['model_availability']['success']}")
    print(f"Buffer Working: {results['emotion_buffer']['success']}")
    print(f"Risk Assessment Working: {results['risk_assessment']['success']}")
    print(f"Acoustic Features Working: {results['acoustic_features']['success']}")
    
    if results['model_availability']['success']:
        model_results = results['model_availability']['results']
        print(f"Transformers Available: {model_results['transformers_available']}")
        print(f"Model Loaded: {model_results.get('model_loaded', False)}")
        if model_results.get('detected_emotions'):
            print(f"Detected Emotions: {model_results['detected_emotions']}")
    
    # Test plugin creation
    print(f"\n=== Plugin Creation Test ===")
    try:
        plugin = create_emotion_plugin()
        print(f"Plugin created: {plugin.name}")
        print(f"Enabled: {plugin.enabled}")
        
        if plugin.enabled:
            # Test session lifecycle
            session_id = "test_session_emotion_123"
            plugin.on_recording_started(session_id)
            plugin.on_speaker_detected("speaker_001", session_id)
            
            # Simulate some audio chunks with emotions
            test_emotions = ["happy", "neutral", "angry", "sad", "excited"]
            
            for i, emotion in enumerate(test_emotions):
                chunk = AudioChunk(
                    data=np.random.random(1024).astype(np.float32),
                    timestamp=time.time(),
                    sample_rate=16000,
                    speaker_id="speaker_001",
                    consent_token="CT-test123",
                    session_id=session_id,
                    is_speech=True,
                    confidence=0.9,
                    voice_activity_score=0.8
                )
                
                plugin.process_audio(chunk)
                time.sleep(0.5)
            
            # Get emotion overview
            overview = plugin.get_session_emotion_overview()
            print(f"Session overview: {overview}")
            
            # Get speaker emotion state
            speaker_state = plugin.get_speaker_emotion_state("speaker_001")
            print(f"Speaker state: {speaker_state}")
            
            # Test API
            emotion_api = EmotionAPI(plugin)
            risk_analysis = emotion_api.analyze_emotional_risk("speaker_001", 300)
            print(f"Risk analysis: {risk_analysis}")
            
            # Stop session
            plugin.on_recording_stopped(session_id)
            
            # Cleanup
            plugin.cleanup()
            
            print("Plugin test completed successfully")
        else:
            print("Plugin disabled - emotion detection not available")
        
    except Exception as e:
        print(f"Plugin test failed: {e}")
    
    # Test emotion detection on audio file if available
    print(f"\n=== Audio File Test ===")
    audio_file = "test_emotional_audio.wav"  # Replace with actual audio file
    if os.path.exists(audio_file):
        emotion_result = test_emotion_detection(audio_file)
        print(f"Emotion detection test: {emotion_result}")
    else:
        print("No test audio file found - skipping audio emotion test")
    
    print(f"\n=== Test Suite Complete ===")
    
    # Display configuration template
    print(f"\n=== Configuration Template ===")
    print(EMOTION_CONFIG_TEMPLATE)
Displaying Emotion.txt.
